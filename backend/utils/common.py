import unicodedata
import requests
from bs4 import BeautifulSoup
import time
from database.database import save_articles_to_postgres
import re

def normalize_text(text):
    """
    Chu·∫©n h√≥a vƒÉn b·∫£n:
    - X√≥a d·∫•u ti·∫øng Vi·ªát
    - Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    - Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    """
    if not isinstance(text, str):
        return ""

    normalized = unicodedata.normalize('NFKD', text)
    ascii_text = normalized.encode('ASCII', 'ignore').decode('utf-8')
    return " ".join(ascii_text.lower().split())

def clean_text(text: str) -> str:
    """
    L√†m s·∫°ch vƒÉn b·∫£n: xo√° k√Ω t·ª± ƒë·∫∑c bi·ªát, chu·∫©n h√≥a unicode, x√≥a d·∫•u, vi·∫øt th∆∞·ªùng.
    """
    if not text:
        return ""

    # Chu·∫©n ho√° unicode
    text = unicodedata.normalize('NFKC', text)

    # Xo√° d·∫•u c√¢u, k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = re.sub(r"[^\w\s]", " ", text)

    # Xo√° s·ªë
    text = re.sub(r"\d+", " ", text)

    # Xo√° nhi·ªÅu kho·∫£ng tr·∫Øng
    text = re.sub(r"\s+", " ", text)

    return text.strip().lower()


def general_crawl(n, category_mapping, category_urls, site_prefix, selectors, extract_func, table_name):
    all_articles = []

    for category, url in category_urls.items():
        print(f"\nƒêang crawl {n} b√†i t·ª´ chuy√™n m·ª•c {category}...")

        full_articles = []
        seen_urls = set()
        ignored_urls = set()
        attempts = 0

        while len(full_articles) < n:
            print(f"\nüîÑ Th·ª≠ l·∫ßn {attempts + 1} ƒë·ªÉ l·∫•y ƒë·ªß b√†i h·ª£p l·ªá...")

            try:
                res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
                if res.status_code != 200:
                    print("‚ùå Kh√¥ng th·ªÉ truy c·∫≠p danh m·ª•c.")
                    break
                soup = BeautifulSoup(res.text, "lxml")
                article_links = soup.select(selectors['article_links'])

                articles = []
                for link in article_links:
                    title = link.get_text(strip=True)
                    full_url = site_prefix + link['href']
                    if full_url not in seen_urls:
                        articles.append({"title": title, "url": full_url})
                        seen_urls.add(full_url)
                    if len(articles) >= n:
                        break
            except Exception as e:
                print(f"‚ùå L·ªói khi l·∫•y danh s√°ch b√†i: {e}")
                break

            if not articles:
                print(f"‚ö† Kh√¥ng t√¨m th·∫•y b√†i n√†o trong chuy√™n m·ª•c {category}.")
                break

            for article in articles:
                if article["url"] in ignored_urls:
                    continue

                content = extract_func(article["url"], category, category_mapping)
                if content:
                    full_articles.append(content)
                    if len(full_articles) >= n:
                        break
                else:
                    ignored_urls.add(article["url"])

            attempts += 1

        if full_articles:
            print(f"L∆∞u {len(full_articles)} b√†i v√†o b·∫£ng {table_name} (category: {category})")
            save_articles_to_postgres(full_articles, table_name, category)
            all_articles.extend(full_articles)
        else:
            print(f"Kh√¥ng c√≥ b√†i h·ª£p l·ªá n√†o ƒë·ªÉ l∆∞u cho chuy√™n m·ª•c {category}.")

    return all_articles
