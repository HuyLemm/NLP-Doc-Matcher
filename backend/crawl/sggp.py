import requests
from bs4 import BeautifulSoup
from database.database import save_articles_to_postgres
from utils.common import normalize_text
import time

CATEGORY_MAPPING = {
    "chinhtri": "Ch√≠nh tr·ªã",
    "xahoi": "X√£ h·ªôi",
    "phapluat": "Ph√°p lu·∫≠t",
    "kinhte": "Kinh t·∫ø",
    "thegioi": "Th·∫ø gi·ªõi",
    "giaoduc": "Gi√°o d·ª•c",
    "yte_suckhoe": "Y t·∫ø - S·ª©c kh·ªèe",
    "vanhoa_giaitri": "VƒÉn h√≥a - Gi·∫£i tr√≠",
    "nhipcau_bandoc": "Nh·ªãp c·∫ßu b·∫°n ƒë·ªçc",
    "khoahoc_congnghe": "Khoa h·ªçc - C√¥ng ngh·ªá"
}

CATEGORY_URLS = {
    "chinhtri": "https://www.sggp.org.vn/chinhtri/",
    "xahoi": "https://www.sggp.org.vn/xahoi/",
    "phapluat": "https://www.sggp.org.vn/phapluat/",
    "kinhte": "https://www.sggp.org.vn/kinhte/",
    "thegioi": "https://www.sggp.org.vn/thegioi/",
    "giaoduc": "https://www.sggp.org.vn/giaoduc/",
    "yte_suckhoe": "https://www.sggp.org.vn/yte-suckhoe/",
    "vanhoa_giaitri": "https://www.sggp.org.vn/vanhoa-giaitri/",
    "nhipcau_bandoc": "https://www.sggp.org.vn/nhipcau-bandoc/",
    "khoahoc_congnghe": "https://www.sggp.org.vn/khoahoc-congnghe/"
}

def get_articles_list(category_url, seen_urls, n):
    """L·∫•y danh s√°ch b√†i vi·∫øt t·ª´ `abf-cate`, n·∫øu ch∆∞a ƒë·ªß th√¨ l·∫•y t·ª´ `zone-2`."""
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(category_url, headers=headers)
    if response.status_code != 200:
        print("‚ùå Kh√¥ng th·ªÉ truy c·∫≠p danh m·ª•c.")
        return []

    soup = BeautifulSoup(response.text, "lxml")
    articles = []

    # üü¢ L·∫•y b√†i t·ª´ `abf-cate`
    box_top_links = soup.select("div.abf-cate a")
    for link in box_top_links:
        title = link.get_text(strip=True)
        url = "https://www.sggp.org.vn" + link["href"]

        if url not in seen_urls:
            articles.append({"title": title, "url": url})
            seen_urls.add(url)

        if len(articles) >= n:
            return articles

    # üîé N·∫øu ch∆∞a ƒë·ªß b√†i, l·∫•y t·ª´ `zone-2`
    list_main_links = soup.select("div.zone-2 a")
    for link in list_main_links:
        title = link.get_text(strip=True)
        url = "https://www.sggp.org.vn" + link["href"]

        if url not in seen_urls:
            articles.append({"title": title, "url": url})
            seen_urls.add(url)

        if len(articles) >= n:
            break

    return articles

def get_article_content(url, expected_category):
    """L·∫•y n·ªôi dung b√†i vi·∫øt t·ª´ b√°o S√†i G√≤n Gi·∫£i Ph√≥ng v√† ki·ªÉm tra th·ªÉ lo·∫°i"""
    headers = {"User-Agent": "Mozilla/5.0"}

    # Th·ª≠ l·∫°i 3 l·∫ßn n·∫øu g·∫∑p l·ªói k·∫øt n·ªëi
    for attempt in range(3):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                print(f"Kh√¥ng th·ªÉ t·∫£i b√†i vi·∫øt: {url}")
                return None
            break
        except requests.exceptions.RequestException:
            print(f"K·∫øt n·ªëi b·ªã l·ªói, th·ª≠ l·∫°i l·∫ßn {attempt+1}...")
            time.sleep(2)
    else:
        return None

    soup = BeautifulSoup(response.text, "lxml")

    # üü¢ L·∫•y th·ªÉ lo·∫°i b√†i vi·∫øt (·∫¢nh 2)
    category_element = soup.select_one("div.breadcrumbs a")
    category = category_element.get_text(strip=True) if category_element else "Kh√¥ng r√µ th·ªÉ lo·∫°i"

    # üü¢ L·∫•y ti√™u ƒë·ªÅ b√†i vi·∫øt (·∫¢nh 3)
    title = soup.find("h1", class_="article_title cms-title")
    title = title.get_text(strip=True) if title else "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"

    # üü¢ L·∫•y t√°c gi·∫£ & th·ªùi gian (·∫¢nh 4)
    author = soup.select_one("span.author.cms-source")
    author = author.get_text(strip=True) if author else "Kh√¥ng r√µ t√°c gi·∫£"

    date = soup.select_one("time.time")
    date = date.get_text(strip=True) if date else "Kh√¥ng r√µ ng√†y"

    # Debug: In th·ªÉ lo·∫°i ƒë·ªÉ ki·ªÉm tra
    print(f"ƒê√£ l·∫•y th·ªÉ lo·∫°i t·ª´ b√†i vi·∫øt: {category} - {url}")

    # Chu·∫©n h√≥a th·ªÉ lo·∫°i tr∆∞·ªõc khi so s√°nh
    normalized_category = normalize_text(category)
    normalized_expected = normalize_text(CATEGORY_MAPPING.get(expected_category, expected_category))

    # N·∫øu th·ªÉ lo·∫°i kh√¥ng kh·ªõp chuy√™n m·ª•c => B·ªé QUA
    if normalized_category != normalized_expected:
        print(f"B·ªè qua b√†i vi·∫øt '{title}' v√¨ th·ªÉ lo·∫°i {category} kh√¥ng kh·ªõp v·ªõi chuy√™n m·ª•c {expected_category}!")
        return None

    # üü¢ L·∫•y n·ªôi dung b√†i vi·∫øt (·∫¢nh 5)
    content_elements = soup.select("div.article_body.cms-body p")
    content = "\n".join([p.get_text(strip=True) for p in content_elements if p.get_text(strip=True)])

    return {
        "title": title,
        "author": author,
        "date": date,
        "category": category,
        "content": content,
        "url": url
    }

def crawl_sggp(n):
    for category, url in CATEGORY_URLS.items():
        print(f"\nƒêang crawl {n} b√†i t·ª´ chuy√™n m·ª•c {category}...")

        full_articles = []
        seen_urls = set()
        ignored_articles = set()  
        attempts = 0  

        while len(full_articles) < n:
            print(f"\nüîÑ Th·ª≠ l·∫ßn {attempts + 1} ƒë·ªÉ l·∫•y ƒë·ªß b√†i h·ª£p l·ªá...")

            articles = get_articles_list(url, seen_urls, n)
            if not articles:
                print(f"‚ö† Kh√¥ng t√¨m th·∫•y th√™m b√†i n√†o trong chuy√™n m·ª•c {category}. D·ª´ng t√¨m ki·∫øm.")
                break

            for article in articles:
                if article["url"] in ignored_articles:
                    continue  

                content = get_article_content(article["url"], category) 
                if content:
                    print(f"L∆∞u b√†i vi·∫øt: {content['title']} - Th·ªÉ lo·∫°i: {content['category']}")
                    full_articles.append(content)
                    if len(full_articles) >= n:
                        break  
                else:
                    ignored_articles.add(article["url"])  

            attempts += 1
            if len(full_articles) < n:
                print(f"Ch∆∞a ƒë·ªß b√†i h·ª£p l·ªá ({len(full_articles)}/{n}), ti·∫øp t·ª•c t√¨m ki·∫øm...")
            else:
                break  

        if full_articles:
            print(f"L∆∞u {len(full_articles)} b√†i v√†o b·∫£ng sggp_articles (category: {category})")
            save_articles_to_postgres(full_articles, "sggp_articles", category)
            return full_articles
        else:
            print(f"Kh√¥ng c√≥ b√†i h·ª£p l·ªá n√†o ƒë·ªÉ l∆∞u cho chuy√™n m·ª•c {category}.")
