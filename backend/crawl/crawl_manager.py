import threading
from crawl.nld import crawl_nld
from crawl.sggp import crawl_sggp
from crawl.thanhnien import crawl_thanhnien
from crawl.tuoitre import crawl_tuoitre
from database.database import save_articles_to_postgres

# √Ånh x·∫° ngu·ªìn b√°o v·ªõi script t∆∞∆°ng ·ª©ng
NEWS_SOURCES = {
    "tuoitre": crawl_tuoitre,
    "thanhnien": crawl_thanhnien,
    "nld": crawl_nld,
    "sggp": crawl_sggp
}

def crawl_news(source, num_articles):
    if source not in NEWS_SOURCES:
        print(f"‚ùå B√°o '{source}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")
        return []

    print(f"üîç ƒêang crawl {num_articles} b√†i t·ª´ {source}...")
    articles = NEWS_SOURCES[source](num_articles)

    if articles:
        save_articles_to_postgres(articles, f"{source}_articles", source)
        print(f"‚úÖ ƒê√£ l∆∞u {len(articles)} b√†i v√†o PostgreSQL.")
    else:
        print(f"‚ö† Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë∆∞·ª£c crawl t·ª´ {source}.")

    return articles  # ‚úÖ TR·∫¢ V·ªÄ K·∫æT QU·∫¢


def start_crawling(sources, num_articles):
    all_articles = []

    def threaded_crawl(src):
        articles = crawl_news(src, num_articles)
        if articles:
            all_articles.extend(articles)

    threads = [threading.Thread(target=threaded_crawl, args=(src,)) for src in sources]
    for thread in threads:
        thread.start()
    for thread in threads:
        thread.join()

    return all_articles


