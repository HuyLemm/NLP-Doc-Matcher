import requests
from bs4 import BeautifulSoup
from database.database import save_articles_to_postgres
from utils.common import normalize_text
import time

# Ãnh xáº¡ chuyÃªn má»¥c URL vá»›i thá»ƒ loáº¡i thá»±c táº¿ trÃªn Thanh NiÃªn
CATEGORY_MAPPING = {
    "chinhtri": "ChÃ­nh trá»‹",
    "thoisu": "Thá»i sá»±",
    "thegioi": "Tháº¿ giá»›i",
    "kinhte": "Kinh táº¿",
    "doisong": "Äá»i sá»‘ng",
    "suckhoe": "Sá»©c khá»e",
    "gioitre": "Giá»›i tráº»",
    "giaoduc": "GiÃ¡o dá»¥c",
    "dulich": "Du lá»‹ch",
    "vanhoa": "VÄƒn hÃ³a",
    "giaitri": "Giáº£i trÃ­",
    "thethao": "Thá»ƒ thao",
    "congnghe": "CÃ´ng nghá»‡",
    "xe": "Xe",
}

# Danh sÃ¡ch chuyÃªn má»¥c trÃªn Thanh NiÃªn
CATEGORY_URLS = {
    "chinhtri": "https://thanhnien.vn/chinh-tri.htm",
    "thoisu": "https://thanhnien.vn/thoi-su.htm",
    "thegioi": "https://thanhnien.vn/the-gioi.htm",
    "kinhte": "https://thanhnien.vn/kinh-te.htm",
    "doisong": "https://thanhnien.vn/doi-song.htm",
    "suckhoe": "https://thanhnien.vn/suc-khoe.htm",
    "gioitre": "https://thanhnien.vn/gioi-tre.htm",
    "giaoduc": "https://thanhnien.vn/giao-duc.htm",
    "dulich": "https://thanhnien.vn/du-lich.htm",
    "vanhoa": "https://thanhnien.vn/van-hoa.htm",
    "giaitri": "https://thanhnien.vn/giai-tri.htm",
    "thethao": "https://thanhnien.vn/the-thao.htm",
    "congnghe": "https://thanhnien.vn/cong-nghe.htm",
    "xe": "https://thanhnien.vn/xe.htm",
}

def get_articles_list(category_url, seen_urls, n):
    """Láº¥y danh sÃ¡ch bÃ i viáº¿t tá»« box-top trÆ°á»›c, náº¿u chÆ°a Ä‘á»§ thÃ¬ láº¥y tá»« list-listing-main"""
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(category_url, headers=headers)
    if response.status_code != 200:
        print("KhÃ´ng thá»ƒ truy cáº­p danh má»¥c.")
        return []

    soup = BeautifulSoup(response.text, "lxml")

    articles = []

    # ğŸŸ¢ Láº¥y bÃ i tá»« box-top trÆ°á»›c
    box_top_links = soup.select("div.box-category-item-main a.box-category-link-title")
    for link in box_top_links:
        title = link.get_text(strip=True)
        url = "https://thanhnien.vn" + link["href"]

        if url not in seen_urls:
            articles.append({"title": title, "url": url})
            seen_urls.add(url)

        if len(articles) >= n:  # Dá»«ng ngay náº¿u Ä‘á»§ bÃ i
            return articles

    # ğŸŸ¢ Náº¿u chÆ°a Ä‘á»§ bÃ i, láº¥y thÃªm tá»« list-listing-flex
    if len(articles) < n:
        list_main_links = soup.select("div.list__stream-flex div.list__stream-main a.box-category-link-title")
        for link in list_main_links:
            title = link.get_text(strip=True)
            url = "https://thanhnien.vn" + link["href"]

            if url not in seen_urls:
                articles.append({"title": title, "url": url})
                seen_urls.add(url)

            if len(articles) >= n:  # Dá»«ng ngay náº¿u Ä‘á»§ bÃ i
                break

    return articles

def get_article_content(url, expected_category):
    """Láº¥y ná»™i dung bÃ i viáº¿t vÃ  kiá»ƒm tra thá»ƒ loáº¡i cÃ³ khá»›p vá»›i chuyÃªn má»¥c khÃ´ng"""
    headers = {"User-Agent": "Mozilla/5.0"}

    # Thá»­ láº¡i 3 láº§n náº¿u gáº·p lá»—i káº¿t ná»‘i
    for attempt in range(3):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                print(f"KhÃ´ng thá»ƒ táº£i bÃ i viáº¿t: {url}")
                return None
            break  # Náº¿u táº£i thÃ nh cÃ´ng thÃ¬ thoÃ¡t vÃ²ng láº·p
        except requests.exceptions.RequestException:
            print(f"Káº¿t ná»‘i bá»‹ lá»—i, thá»­ láº¡i láº§n {attempt+1}...")
            time.sleep(2)  # Chá» 2 giÃ¢y rá»“i thá»­ láº¡i láº§n ná»¯a
    else:
        return None  # Náº¿u sau 3 láº§n váº«n lá»—i thÃ¬ bá» bÃ i nÃ y

    soup = BeautifulSoup(response.text, "lxml")

    # Láº¥y tiÃªu Ä‘á»
    title = soup.find("h1", class_="detail-title").get_text(strip=True) if soup.find("h1", class_="detail-title") else "KhÃ´ng cÃ³ tiÃªu Ä‘á»"

    # Láº¥y tÃ¡c giáº£
    author = soup.select_one("div.detail-author a.name").get_text(strip=True) if soup.select_one("div.detail-author a.name") else "KhÃ´ng rÃµ tÃ¡c giáº£"

    # Láº¥y ngÃ y Ä‘Äƒng
    date = soup.select_one("div.detail-time").get_text(strip=True) if soup.select_one("div.detail-time") else "KhÃ´ng rÃµ ngÃ y"

    # Láº¥y thá»ƒ loáº¡i chÃ­nh xÃ¡c
    category_element = soup.select_one("div.detail-cate a")
    category = category_element.get_text(strip=True) if category_element else "KhÃ´ng rÃµ thá»ƒ loáº¡i"

    # Debug: In thá»ƒ loáº¡i Ä‘á»ƒ kiá»ƒm tra
    print(f"ÄÃ£ láº¥y thá»ƒ loáº¡i tá»« bÃ i viáº¿t: {category} - {url}")

    # Chuáº©n hÃ³a thá»ƒ loáº¡i trÆ°á»›c khi so sÃ¡nh
    normalized_category = normalize_text(category)
    normalized_expected = normalize_text(CATEGORY_MAPPING.get(expected_category, expected_category))

    # Náº¿u thá»ƒ loáº¡i khÃ´ng khá»›p chuyÃªn má»¥c => Bá» QUA
    if normalized_category != normalized_expected:
        print(f"Bá» qua bÃ i viáº¿t '{title}' vÃ¬ thá»ƒ loáº¡i {category} khÃ´ng khá»›p vá»›i chuyÃªn má»¥c {expected_category}!")
        return None

    # Láº¥y ná»™i dung bÃ i viáº¿t
    content_elements = soup.select("div.detail-content[data-role='content'] p")
    content = "\n".join([p.get_text(strip=True) for p in content_elements if p.get_text(strip=True)])

    return {
        "title": title,
        "author": author,
        "date": date,
        "category": category,  # Tráº£ vá» thá»ƒ loáº¡i chÃ­nh xÃ¡c tá»« bÃ i viáº¿t
        "content": content,
        "url": url
    }


def crawl_thanhnien(n):
    all_articles =[]

    for category, url in CATEGORY_URLS.items():
        print(f"\nÄang crawl {n} bÃ i tá»« chuyÃªn má»¥c {category}...")

        full_articles = []
        seen_urls = set()
        ignored_articles = set()  
        attempts = 0  

        while len(full_articles) < n:
            print(f"\nğŸ”„ Thá»­ láº§n {attempts + 1} Ä‘á»ƒ láº¥y Ä‘á»§ bÃ i há»£p lá»‡...")

            articles = get_articles_list(url, seen_urls, n)
            if not articles:
                print(f"âš  KhÃ´ng tÃ¬m tháº¥y thÃªm bÃ i nÃ o trong chuyÃªn má»¥c {category}. Dá»«ng tÃ¬m kiáº¿m.")
                break

            for article in articles:
                if article["url"] in ignored_articles:
                    continue  

                content = get_article_content(article["url"], category) 
                if content:
                    print(f"LÆ°u bÃ i viáº¿t: {content['title']} - Thá»ƒ loáº¡i: {content['category']}")
                    full_articles.append(content)
                    if len(full_articles) >= n:
                        break  
                else:
                    ignored_articles.add(article["url"])  

            attempts += 1
            if len(full_articles) < n:
                print(f"ChÆ°a Ä‘á»§ bÃ i há»£p lá»‡ ({len(full_articles)}/{n}), tiáº¿p tá»¥c tÃ¬m kiáº¿m...")
            else:
                break  

        if full_articles:
            print(f"LÆ°u {len(full_articles)} bÃ i vÃ o báº£ng thanhnien_articles (category: {category})")
            save_articles_to_postgres(full_articles, "thanhnien_articles", category)
            all_articles.extend(full_articles)
        else:
            print(f"KhÃ´ng cÃ³ bÃ i há»£p lá»‡ nÃ o Ä‘á»ƒ lÆ°u cho chuyÃªn má»¥c {category}.")

    return all_articles 

