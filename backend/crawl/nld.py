import requests
from bs4 import BeautifulSoup
from database.database import save_articles_to_postgres
from utils.common import normalize_text
import time

# √Ånh x·∫° chuy√™n m·ª•c URL v·ªõi th·ªÉ lo·∫°i th·ª±c t·∫ø tr√™n B√°o Ng∆∞·ªùi Lao ƒê·ªông
CATEGORY_MAPPING = {
    "thoisu": "Th·ªùi s·ª±",
    "quocte": "Qu·ªëc t·∫ø",
    "laodong": "Lao ƒë·ªông",
    "bandoc": "B·∫°n ƒë·ªçc",
    "netzero": "Net Zero",
    "kinhte": "Kinh t·∫ø",
    "suckhoe": "S·ª©c kh·ªèe",
    "giaoduc": "Gi√°o d·ª•c",
    "phapluat": "Ph√°p lu·∫≠t",
    "vanhoa_vannghe": "VƒÉn h√≥a - VƒÉn ngh·ªá",
    "giaitri": "Gi·∫£i tr√≠",
    "thethao": "Th·ªÉ thao",
    "ai365": "AI 365",
    "giadinh": "Gia ƒë√¨nh"
}

# Danh s√°ch chuy√™n m·ª•c tr√™n B√°o Ng∆∞·ªùi Lao ƒê·ªông
CATEGORY_URLS = {
    "thoisu": "https://nld.com.vn/thoi-su.htm",
    "quocte": "https://nld.com.vn/quoc-te.htm",
    "laodong": "https://nld.com.vn/lao-dong.htm",
    "bandoc": "https://nld.com.vn/ban-doc.htm",
    "netzero": "https://nld.com.vn/net-zero.htm",
    "kinhte": "https://nld.com.vn/kinh-te.htm",
    "suckhoe": "https://nld.com.vn/suc-khoe.htm",
    "giaoduc": "https://nld.com.vn/giao-duc-khoa-hoc.htm",
    "phapluat": "https://nld.com.vn/phap-luat.htm",
    "vanhoa_vannghe": "https://nld.com.vn/van-hoa-van-nghe.htm",
    "giaitri": "https://nld.com.vn/giai-tri.htm",
    "thethao": "https://nld.com.vn/the-thao.htm",
    "ai365": "https://nld.com.vn/ai-365.htm",
    "giadinh": "https://nld.com.vn/gia-dinh.htm"
}



def get_articles_list(category_url, seen_urls, n):
    """L·∫•y danh s√°ch b√†i vi·∫øt t·ª´ box-category-middle tr∆∞·ªõc, n·∫øu ch∆∞a ƒë·ªß th√¨ l·∫•y t·ª´ list__news-flex"""
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(category_url, headers=headers)
    if response.status_code != 200:
        print("‚ùå Kh√¥ng th·ªÉ truy c·∫≠p danh m·ª•c.")
        return []

    soup = BeautifulSoup(response.text, "lxml")
    articles = []

    # üü¢ L·∫•y b√†i t·ª´ `box-category-middle` tr∆∞·ªõc
    box_top_links = soup.select("div.box-category-middle a.box-category-link-title")
    for link in box_top_links:
        title = link.get_text(strip=True)
        url = "https://nld.com.vn" + link["href"]

        if url not in seen_urls:
            articles.append({"title": title, "url": url})
            seen_urls.add(url)

        if len(articles) >= n:  # D·ª´ng ngay n·∫øu ƒë·ªß b√†i
            return articles

    # üîé N·∫øu ch∆∞a ƒë·ªß b√†i, l·∫•y t·ª´ `list__news-flex`
    list_main_links = soup.select("div.list__news-flex div.list__news-item a.box-category-link-title")

    for link in list_main_links:
        title = link.get_text(strip=True)
        url = "https://nld.com.vn" + link["href"]

        if url not in seen_urls:
            articles.append({"title": title, "url": url})
            seen_urls.add(url)

        if len(articles) >= n:  # D·ª´ng ngay n·∫øu ƒë·ªß b√†i
            break

    return articles


def get_article_content(url, expected_category):
    """L·∫•y n·ªôi dung b√†i vi·∫øt t·ª´ b√°o Ng∆∞·ªùi Lao ƒê·ªông v√† ki·ªÉm tra th·ªÉ lo·∫°i"""
    headers = {"User-Agent": "Mozilla/5.0"}

    # Th·ª≠ l·∫°i 3 l·∫ßn n·∫øu g·∫∑p l·ªói k·∫øt n·ªëi
    for attempt in range(3):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                print(f"Kh√¥ng th·ªÉ t·∫£i b√†i vi·∫øt: {url}")
                return None
            break  # N·∫øu t·∫£i th√†nh c√¥ng th√¨ tho√°t v√≤ng l·∫∑p
        except requests.exceptions.RequestException:
            print(f"K·∫øt n·ªëi b·ªã l·ªói, th·ª≠ l·∫°i l·∫ßn {attempt+1}...")
            time.sleep(2)
    else:
        return None  # N·∫øu sau 3 l·∫ßn v·∫´n l·ªói th√¨ b·ªè b√†i n√†y

    soup = BeautifulSoup(response.text, "lxml")

    # üü¢ L·∫•y ti√™u ƒë·ªÅ b√†i vi·∫øt (·∫¢nh 5)
    title = soup.find("h1", class_="detail-title")
    title = title.get_text(strip=True) if title else "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"

    # üü¢ L·∫•y t√°c gi·∫£ (·∫¢nh 2)
    author = soup.select_one("div.detail-author p.name[data-role='author']")
    author = author.get_text(strip=True) if author else "Kh√¥ng r√µ t√°c gi·∫£"

    # üü¢ L·∫•y ng√†y ƒëƒÉng b√†i vi·∫øt (·∫¢nh 3)
    date = soup.select_one("div.detail-time div[data-role='publishdate']")
    date = date.get_text(strip=True) if date else "Kh√¥ng r√µ ng√†y"

    # üü¢ L·∫•y th·ªÉ lo·∫°i b√†i vi·∫øt (·∫¢nh 1)
    category_element = soup.select_one("div.detail-cate a.category-name_ac")
    category = category_element.get_text(strip=True) if category_element else "Kh√¥ng r√µ th·ªÉ lo·∫°i"

    # Debug: In th·ªÉ lo·∫°i ƒë·ªÉ ki·ªÉm tra
    print(f"ƒê√£ l·∫•y th·ªÉ lo·∫°i t·ª´ b√†i vi·∫øt: {category} - {url}")

    # Chu·∫©n h√≥a th·ªÉ lo·∫°i tr∆∞·ªõc khi so s√°nh
    normalized_category = normalize_text(category)
    normalized_expected = normalize_text(CATEGORY_MAPPING.get(expected_category, expected_category))

    # N·∫øu th·ªÉ lo·∫°i kh√¥ng kh·ªõp chuy√™n m·ª•c => B·ªé QUA
    if normalized_category != normalized_expected:
        print(f"B·ªè qua b√†i vi·∫øt '{title}' v√¨ th·ªÉ lo·∫°i {category} kh√¥ng kh·ªõp v·ªõi chuy√™n m·ª•c {expected_category}!")
        return None

    # üü¢ L·∫•y n·ªôi dung b√†i vi·∫øt (·∫¢nh 4)
    content_elements = soup.select("div.detail-content.afcbc-body[data-role='content'] p")
    content = "\n".join([p.get_text(strip=True) for p in content_elements if p.get_text(strip=True)])

    return {
        "title": title,
        "author": author,
        "date": date,
        "category": category,  # Tr·∫£ v·ªÅ th·ªÉ lo·∫°i ch√≠nh x√°c t·ª´ b√†i vi·∫øt
        "content": content,
        "url": url
    }

def crawl_nld(n):
    all_articles = []

    for category, url in CATEGORY_URLS.items():
        print(f"\nƒêang crawl {n} b√†i t·ª´ chuy√™n m·ª•c {category}...")

        full_articles = []
        seen_urls = set()
        ignored_articles = set()
        attempts = 0

        while len(full_articles) < n:
            print(f"\nüîÑ Th·ª≠ l·∫ßn {attempts + 1} ƒë·ªÉ l·∫•y ƒë·ªß b√†i h·ª£p l·ªá...")

            articles = get_articles_list(url, seen_urls, n)
            if not articles:
                print(f"‚ö† Kh√¥ng t√¨m th·∫•y th√™m b√†i n√†o trong chuy√™n m·ª•c {category}. D·ª´ng t√¨m ki·∫øm.")
                break

            for article in articles:
                if article["url"] in ignored_articles:
                    continue

                content = get_article_content(article["url"], category)
                if content:
                    print(f"L∆∞u b√†i vi·∫øt: {content['title']} - Th·ªÉ lo·∫°i: {content['category']}")
                    full_articles.append(content)
                    if len(full_articles) >= n:
                        break
                else:
                    ignored_articles.add(article["url"])

            attempts += 1
            if len(full_articles) < n:
                print(f"Ch∆∞a ƒë·ªß b√†i h·ª£p l·ªá ({len(full_articles)}/{n}), ti·∫øp t·ª•c t√¨m ki·∫øm...")
            else:
                break

        if full_articles:
            print(f"L∆∞u {len(full_articles)} b√†i v√†o b·∫£ng nld_articles (category: {category})")
            save_articles_to_postgres(full_articles, "nld_articles", category)
            all_articles.extend(full_articles)  # ‚úÖ G·ªôp v√†o t·ªïng
        else:
            print(f"Kh√¥ng c√≥ b√†i h·ª£p l·ªá n√†o ƒë·ªÉ l∆∞u cho chuy√™n m·ª•c {category}.")

    return all_articles  # ‚úÖ Tr·∫£ v·ªÅ to√†n b·ªô b√†i ƒë√£ l∆∞u


