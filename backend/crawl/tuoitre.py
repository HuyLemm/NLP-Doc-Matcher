import requests
from bs4 import BeautifulSoup
# from selenium import webdriver
# from selenium.webdriver.chrome.service import Service
# from webdriver_manager.chrome import ChromeDriverManager
from database.database import save_articles_to_postgres
from utils.common import normalize_text
import time

# Ãnh xáº¡ chuyÃªn má»¥c URL vá»›i thá»ƒ loáº¡i thá»±c táº¿ trÃªn Tuá»•i Tráº»
CATEGORY_MAPPING = {
    "thoisu": "Thá»i sá»±",
    "thegioi": "Tháº¿ giá»›i",
    "phapluat": "PhÃ¡p luáº­t",
    "kinhdoanh": "Kinh doanh",
    "congnghe": "CÃ´ng nghá»‡",
    "xe": "Xe",
    "dulich": "Du lá»‹ch",
    "nhipsongtre": "Nhá»‹p sá»‘ng tráº»",
    "vanhoa": "VÄƒn hÃ³a",
    "giaitri": "Giáº£i trÃ­",
    "thethao": "Thá»ƒ thao",
    "giaoduc": "GiÃ¡o dá»¥c",
    "nhadat": "NhÃ  Ä‘áº¥t",
    "suckhoe": "Sá»©c khá»e",
    "giathat": "Giáº£ - Tháº­t",
    "bandoc": "Báº¡n Ä‘á»c lÃ m bÃ¡o"
}

# Danh sÃ¡ch chuyÃªn má»¥c trÃªn Tuá»•i Tráº»
CATEGORY_URLS = {
    "thoisu": "https://tuoitre.vn/thoi-su.htm",
    "thegioi": "https://tuoitre.vn/the-gioi.htm",
    "phapluat": "https://tuoitre.vn/phap-luat.htm",
    "kinhdoanh": "https://tuoitre.vn/kinh-doanh.htm",
    "congnghe": "https://tuoitre.vn/cong-nghe.htm",
    "xe": "https://tuoitre.vn/xe.htm",
    "dulich": "https://tuoitre.vn/du-lich.htm",
    "nhipsongtre": "https://tuoitre.vn/nhip-song-tre.htm",
    "vanhoa": "https://tuoitre.vn/van-hoa.htm",
    "giaitri": "https://tuoitre.vn/giai-tri.htm",
    "thethao": "https://tuoitre.vn/the-thao.htm",
    "giaoduc": "https://tuoitre.vn/giao-duc.htm",
    "nhadat": "https://tuoitre.vn/nha-dat.htm",
    "suckhoe": "https://tuoitre.vn/suc-khoe.htm",
    "giathat": "https://tuoitre.vn/gia-that.htm",
    "bandoc": "https://tuoitre.vn/ban-doc-lam-bao.htm"
}


# def get_dynamic_html(url):
#     """DÃ¹ng Selenium Ä‘á»ƒ táº£i HTML sau khi JavaScript cháº¡y"""
#     options = webdriver.ChromeOptions()
#     options.add_argument("--headless")  # Cháº¡y khÃ´ng hiá»ƒn thá»‹ trÃ¬nh duyá»‡t
#     options.add_argument("--disable-gpu")  
#     options.add_argument("--no-sandbox")  
#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

#     driver.get(url)
#     time.sleep(5)  # Chá» trang táº£i xong

#     html = driver.page_source
#     driver.quit()
#     return html

# def get_articles_list(category_url, seen_urls, n):
#     """Láº¥y danh sÃ¡ch bÃ i viáº¿t tá»« box-top trÆ°á»›c, náº¿u chÆ°a Ä‘á»§ thÃ¬ láº¥y tá»« list__listing-main"""
#     headers = {"User-Agent": "Mozilla/5.0"}
#     response = requests.get(category_url, headers=headers)
#     if response.status_code != 200:
#         print("KhÃ´ng thá»ƒ truy cáº­p danh má»¥c.")
#         return []

#     soup = BeautifulSoup(response.text, "lxml")
#     articles = []

#     # ğŸŸ¢ Láº¥y bÃ i tá»« box-top trÆ°á»›c
#     box_top_links = soup.select("div.box-top a.box-category-link-title")
#     for link in box_top_links:
#         title = link.get_text(strip=True)
#         url = "https://tuoitre.vn" + link["href"]

#         if url not in seen_urls:
#             articles.append({"title": title, "url": url})
#             seen_urls.add(url)

#         if len(articles) >= n:  # Dá»«ng ngay náº¿u Ä‘á»§ bÃ i
#             return articles

#     # Kiá»ƒm tra `list__listing-main` cÃ³ bÃ i khÃ´ng
#     list_main_links = soup.select("div.list__listing-main div.box-category-item div.box-content-title h3.box-title-text a.box-category-link-title")

#     # ğŸŸ¢ Náº¿u Ä‘Ã£ cÃ³ bÃ i trong HTML, láº¥y luÃ´n
#     if len(list_main_links) > 0:
#         print("âœ… Dá»¯ liá»‡u cÃ³ sáºµn trong HTML, láº¥y bÃ i viáº¿t tá»« `list__listing-main`...")
#     else:
#         print("âš  KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t trong HTML, táº£i láº¡i báº±ng Selenium...")
#         html_content = get_dynamic_html(category_url)
#         soup = BeautifulSoup(html_content, "lxml")
#         list_main_links = soup.select("div.list__listing-main div.box-category-item div.box-content-title h3.box-title-text a.box-category-link-title")

#     # ğŸŸ¢ Láº¥y bÃ i tá»« `list__listing-main`
#     for link in list_main_links:
#         title = link.get_text(strip=True)
#         url = "https://tuoitre.vn" + link["href"]

#         if url not in seen_urls:
#             articles.append({"title": title, "url": url})
#             seen_urls.add(url)

#         if len(articles) >= n:  # Dá»«ng ngay náº¿u Ä‘á»§ bÃ i
#             break

#     return articles


def get_articles_list(category_url, seen_urls, n):
    """Láº¥y danh sÃ¡ch bÃ i viáº¿t tá»« box-top trÆ°á»›c, náº¿u chÆ°a Ä‘á»§ thÃ¬ láº¥y tá»« list-listing-main"""
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(category_url, headers=headers)
    if response.status_code != 200:
        print("KhÃ´ng thá»ƒ truy cáº­p danh má»¥c.")
        return []

    soup = BeautifulSoup(response.text, "lxml")

    articles = []

    # ğŸŸ¢ Láº¥y bÃ i tá»« box-top trÆ°á»›c (ÄÃƒ ÄÃšNG)
    box_top_links = soup.select("div.box-top a.box-category-link-title")
    for link in box_top_links:
        title = link.get_text(strip=True)
        url = "https://tuoitre.vn" + link["href"]

        if url not in seen_urls:
            articles.append({"title": title, "url": url})
            seen_urls.add(url)

        if len(articles) >= n:  # Dá»«ng ngay náº¿u Ä‘á»§ bÃ i
            return articles

    # ğŸŸ¢ Náº¿u chÆ°a Ä‘á»§ bÃ i, láº¥y thÃªm tá»« list-listing-main (Sá»¬A SELECTOR)
    if len(articles) < n:
        list_main_links = soup.select("div.list__listing-flex div.list__listing-sub a.box-category-link-title")
        for link in list_main_links:
            title = link.get_text(strip=True)
            url = "https://tuoitre.vn" + link["href"]

            if url not in seen_urls:
                articles.append({"title": title, "url": url})
                seen_urls.add(url)

            if len(articles) >= n:  # Dá»«ng ngay náº¿u Ä‘á»§ bÃ i
                break

    return articles

def get_article_content(url, expected_category):
    """Láº¥y ná»™i dung bÃ i viáº¿t vÃ  kiá»ƒm tra thá»ƒ loáº¡i cÃ³ khá»›p vá»›i chuyÃªn má»¥c khÃ´ng"""
    headers = {"User-Agent": "Mozilla/5.0"}

    # Thá»­ láº¡i 3 láº§n náº¿u gáº·p lá»—i káº¿t ná»‘i
    for attempt in range(3):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                print(f"KhÃ´ng thá»ƒ táº£i bÃ i viáº¿t: {url}")
                return None
            break  # Náº¿u táº£i thÃ nh cÃ´ng thÃ¬ thoÃ¡t vÃ²ng láº·p
        except requests.exceptions.RequestException:
            print(f"Káº¿t ná»‘i bá»‹ lá»—i, thá»­ láº¡i láº§n {attempt+1}...")
            time.sleep(2)  # Chá» 2 giÃ¢y rá»“i thá»­ láº¡i láº§n ná»¯a
    else:
        return None  # Náº¿u sau 3 láº§n váº«n lá»—i thÃ¬ bá» bÃ i nÃ y

    soup = BeautifulSoup(response.text, "lxml")

    # Láº¥y tiÃªu Ä‘á»
    title = soup.find("h1", class_="detail-title").get_text(strip=True) if soup.find("h1", class_="detail-title") else "KhÃ´ng cÃ³ tiÃªu Ä‘á»"

    # Láº¥y tÃ¡c giáº£
    author = soup.select_one("div.detail-author a.name").get_text(strip=True) if soup.select_one("div.detail-author a.name") else "KhÃ´ng rÃµ tÃ¡c giáº£"

    # Láº¥y ngÃ y Ä‘Äƒng
    date = soup.select_one("div.detail-time").get_text(strip=True) if soup.select_one("div.detail-time") else "KhÃ´ng rÃµ ngÃ y"

    # Láº¥y thá»ƒ loáº¡i chÃ­nh xÃ¡c
    category_element = soup.select_one("div.detail-cate a")
    category = category_element.get_text(strip=True) if category_element else "KhÃ´ng rÃµ thá»ƒ loáº¡i"

    # Debug: In thá»ƒ loáº¡i Ä‘á»ƒ kiá»ƒm tra
    print(f"ÄÃ£ láº¥y thá»ƒ loáº¡i tá»« bÃ i viáº¿t: {category} - {url}")

    # Chuáº©n hÃ³a thá»ƒ loáº¡i trÆ°á»›c khi so sÃ¡nh
    normalized_category = normalize_text(category)
    normalized_expected = normalize_text(CATEGORY_MAPPING.get(expected_category, expected_category))

    # Náº¿u thá»ƒ loáº¡i khÃ´ng khá»›p chuyÃªn má»¥c => Bá» QUA
    if normalized_category != normalized_expected:
        print(f"Bá» qua bÃ i viáº¿t '{title}' vÃ¬ thá»ƒ loáº¡i {category} khÃ´ng khá»›p vá»›i chuyÃªn má»¥c {expected_category}!")
        return None

    # Láº¥y ná»™i dung bÃ i viáº¿t
    content_elements = soup.select("div.detail-content[data-role='content'] p")
    content = "\n".join([p.get_text(strip=True) for p in content_elements if p.get_text(strip=True)])

    return {
        "title": title,
        "author": author,
        "date": date,
        "category": category,  # Tráº£ vá» thá»ƒ loáº¡i chÃ­nh xÃ¡c tá»« bÃ i viáº¿t
        "content": content,
        "url": url
    }

def crawl_tuoitre(n):
    all_articles = []

    for category, url in CATEGORY_URLS.items():
        print(f"\nÄang crawl {n} bÃ i tá»« chuyÃªn má»¥c {category}...")

        full_articles = []
        seen_urls = set()
        ignored_articles = set()  
        attempts = 0  

        while len(full_articles) < n:
            print(f"\nThá»­ láº§n {attempts + 1} Ä‘á»ƒ láº¥y Ä‘á»§ bÃ i há»£p lá»‡...")

            articles = get_articles_list(url, seen_urls, n)
            if not articles:
                print(f"KhÃ´ng tÃ¬m tháº¥y thÃªm bÃ i nÃ o trong chuyÃªn má»¥c {category}. Dá»«ng tÃ¬m kiáº¿m.")
                break

            for article in articles:
                if article["url"] in ignored_articles:
                    continue  

                content = get_article_content(article["url"], category) 
                if content:
                    print(f"LÆ°u bÃ i viáº¿t: {content['title']} - Thá»ƒ loáº¡i: {content['category']}")
                    full_articles.append(content)
                    if len(full_articles) >= n:
                        break  
                else:
                    ignored_articles.add(article["url"])  

            attempts += 1
            if len(full_articles) < n:
                print(f"ChÆ°a Ä‘á»§ bÃ i há»£p lá»‡ ({len(full_articles)}/{n}), tiáº¿p tá»¥c tÃ¬m kiáº¿m...")
            else:
                break  

        if full_articles:
            print(f"LÆ°u {len(full_articles)} bÃ i vÃ o báº£ng tuoitre_articles (category: {category})")
            save_articles_to_postgres(full_articles, "tuoitre_articles", category)
            all_articles.extend(full_articles)
        else:
            print(f"KhÃ´ng cÃ³ bÃ i há»£p lá»‡ nÃ o Ä‘á»ƒ lÆ°u cho chuyÃªn má»¥c {category}.")
    return all_articles
